# 人工智能

## 人工智能的技术体系

思考 ？什么是软件 ？  
  其实软件就是一行**数据**  
  数据如何产生？
   
## 开发一个软件的流程

1. 需求分析

2. 静态html页面原型  
不带数据视图view  
技术体系：html5、css3、JQuery、aJax、js(框架)-Node.js

2. 数据库建模  
简写为：UML 技术    
一般利用第三方工具    
开发中最重要的技术  
数据库是：**关系**型数据库-----关系就是：项目的业务逻辑  
常用数据库：Mysql Oracle  

4. 后台编码开发阶段
常用java的框架：ssm、ssh、struts、Hibernate、Spring、Springmvc、Mybatis
基于微服务的框架：Springboot、Springcloud 

5. 测试
包括功能测试 、压力测试、黑盒测试 、白盒测试 .....                        
> 面试题：黑盒就是功能测试，不能测试代码核心
>        白盒测试：测试内核 代码-----成本很高！
       
6. 部署到服务器里面  < www >  
3大云服务器：阿里云、腾讯云、七斗云  
云服务器优点：免费维护！


7. 维护 

收集数据--**数据集**--人工智能的核心就是：处理 **数据集**
           
## 机器学习的流程

i  :机器需要思考--算法  
ii :人需要采集数据  
iii:推广数据的采集找出训练数据--产生机器学习的数据集  
## 机器学习的定义


  i：机器学习是一门从**数据**中研究**算法**的**科学**学科。         
  
  ii：人工智能目前面临的最大问题是数据没有办法获取?  
  重点：**数据的来源有哪些？**
>   1. 来源于 项目数据库数据
>   2. 来源于第三方数据---接口数据
>   3. 缓存数据          
  
numpy的核心是多维度空间（矩阵）

## 机器学习理性认识   
输入： x （属性值）  
输出： y  (目标值 )  
f : X -> Y(理想的公式)  
输入数据：历史记录信息  
g : X -> Y(学习得到的最终公式 )   
d={(x1,y1,z1......),(xn,yn,zn......)}--数据集  
 
机器：1+1 = ？  思考 ？机器快还是人快？  
1000\*12121211398394839=？   
1000\*12121211398394839的n次方呢？  
总结：机器学习只能处理业务数据较为繁琐的数据集！不适合处理业务数据简单的数据！  

## 机器学习的概念
对于某给定的*任务T*，在合理的*性能度量方案P*的前提下，某计算机程序可以自主学习任务T的*经验E*；随着提供合适、优质、大量的经验E，该程序对于任务T性  
> 任务T(机器需要做什么？制作山西刀削面)--性能度量方案P(机器如何削面)--经验E(过了一个月，机器总结经验)--下次直接削面（直接执行任务）

算法(T)：根据业务需要和数据特征选择的相关算法,也就是一个数学公式 
> 例如：线性回归、随机森林、决策树、ID3算法、高斯分布....

模型(E)：基于数据和算法构建出来的模型 
> 结合算法 +数据集<给大家提供>   ===构建一个模型

评估/测试(P)：对模型进行评估的策略
>将模型产生的数据集进行 评估 ---为什么要评估数据？

选择最优数据，摈弃有噪点的数据！

## 机器学习框架
**重点**  

1. sciket-learn(Python)  
http://scikit-learn.org/stable/
基于Python的人工智能开发--Python属于轻量级--企业大量使用  
> 可以使用java进行人工智能开发，但是java语言属于重量级语言，不适合研发AI产品。
> > 重量级,轻量级--针对于服务器读取数据的并发！(存疑)
  
  
  
2. Mahout(Hadoop生态圈基于MapReduce)  
http://mahout.apache.org/
基于大数据的人工智能
目前有阿里、jd、等大型B2C项目使用
> 小公司不适合利用,因为数据量不是 很大，成本很高！  

 
3. 基于spark的人工智能  
http://spark.apache.org/         
目前技术要求最高  
处理数据速度比前两个快的
             
## 机器学习的应用商业场景 
1. 个性化推荐：  
个性化指的是根据各种因素来改变用户体验和呈现给用户内容这些因素可能包含**用户的行为数据**和**外部因素**；推荐常指系统向用户呈现一个用户可能感兴趣的物品列表。  
> 用户的行为数据  
> 即用户直接操作的数据  
> 行为数据有价值  
> 在人工智能里面检索实现的原理是**分词器**
       
2. 精准营销：
从用户群众中找出特定的要求的营销对象           
指的是通过挖掘客户群体的软点数据或者客户数据的损失数据来自动匹配相关的数据集
3. 客户细分：
试图将用户群体分为**不同的组**，根据给定的**用户特征**进行客户分组。     
**用户特征**   **不同的组**  的核心思想就是 ：oop

4. 预测建模及分析：  
 根据已有的数据进行建模，并使用得到的模型预测未来。
 扩展：模式识别   
 
## 数据分析、数据挖掘、机器学习的区别
**重点**  

1. 数据分析：  
数据分析是指用适当的统计分析方法对收集的**大量数据**进行分析，并提取**有用的信息**，以及**形成结论**，从而对数据进行**详细的研究**和**概括过程**。在实际工作中，数据分析可帮助人们**做出判断**；--减小数据的误差！（？）
切记：数据分析一般而言可以分为统计分析(基于Python的数据分析)、 探索性数据分析和验证性数据分析（验证数据的合法性、数据的安全性等......）三大类。  
    
2. 数据挖掘：  
一般指从大量的数据中通过算法**搜索隐藏**于其中的信息的过程。  
程序员也不可能通过插件开发获取到<只能利用算法实现搜索隐藏的数据>  
通常 通过统计、检索、机器学习、模式匹配等诸多方法来实现这个过程。
> 隐藏数据指的是用户自己/程序员无法手动获取  
 
3. 机器学习就是 整合   i  和 ii

## 机器学习的分类    
**重点**  

1. 有监督学习  
利用已经建立好的模型model来预测未来的样本数据  
有监督学习的核心思想是给数据加标签，即是标签化数据   
2. 无监督学习  
指的是没有标签化的数据操作，直接通过机器处理数据，但是这样的误差会大，但是可以推导出很多数据的内部结构
> 内部结构指的是数据的筛选操作

3. 半监督学习  
指的是利用少量的已结标注的数据和大量未标注的数据进行操作  
主要考虑如何利用少量的标注样本和大量的未标注样本进行训练和分类的问题。  
半监督学习对于减少标注代价，提高学习机器性能具有非常重大的实际意义。  
SSL的成立依赖于模型假设，主要分为三大类：**平滑假设**、**聚类假设**、**流行假设**； 其中流行假设更具有普片性。  
SSL类型的算法主要分为四大类：**半监督分类、半监督回归、半监督聚类、半监督降维**。   
缺点：抗干扰能力弱，仅适合于实验室环境，其现实意义还没有体现出来；未来的发展主要是聚焦于新模型假设的产生。

## 机器学习由开始到结束的过程
第一步：将数据分类  
通过分类模型，将样本数据集中的样本映射到某个给定的类别中

第二步： 聚类  
通过聚类模型，将样本数据集中的样本分为几个类别，属于同一类别的样本相似性比较大  --误差变小

第三步：回归  
反映了样本数据集中样本的属性值的特性，通过函数表达样本映射的关系来发现属性值之间的依赖关系  --数据的密度值--离散点

第四步： 关联规则  
即可以根据一个数据项的出现推导出其他数据项的出  现频率。  P（A）-----预测未来

## 机器学习深度算法 
* **C4.5** 分类决策树算法，决策树的核心算法，
* **ID3** 算法的改进算法。 
* **CART** 分类与回归树(Classification and Regression Trees) 
* **kNN** K近邻分类算法；如果一个样本在特征空间中的k个最相似的样本中大多数属于某一个 类别，那么该样本也属于该类别 
* **NaiveBayes** 贝叶斯分类模型；该模型比较适合属性相关性比较小的时候，如果属性相关性比较大的时候，决策树模型比贝叶斯分类模型效果好(原因：贝叶斯模型假设属性之间是互不影响的) 
* **SVM** 支持向量机，一种有监督学习的统计学习方法，广泛应用于统计分类和回归分析中。 
* **EM** 最大期望算法，常用于机器学习和计算机视觉中的数据集聚领域 
* **Apriori** 关联规则挖掘算法 
* **K-Means** 聚类算法，功能是将n个对象根据属性特征分为k个分割(k < n); 属于无监督学习 
* **PageRank** Google搜索重要算法之一 
* **AdaBoost** 迭代算法；利用多个分类器进行数据分类

## 人工智能和机器学习的区别

机器学习比较简答,是基于算法的实现  
先学习机器学习/大数据之后学习深度学习很简单  
深度学习是核心，难度较大  

深度学习是机器学习的子类；
> 深度学习是基于**传统的神经网络算法**发展到**多隐层的一种算法体现**。

## 机器学习开发流程
**重点内容**

1. 数据收集/数据采集   

2. 数据**预**处理  


3. **特征**提取  
特征的分类 
> 1. 基于行为特征  
> 2. 数据本身的特征 
              
4. 模型构建 
就是通过很多算法构建一个模型,用于存放数据集得到的结果
 
5. 模型测试评估

6. 投入使用(模型部署与整合) 
模型的部署需要硬件的支持

 
7. **迭代**优化  
需要借助于工具   
优化性能问题
      
## 数据的核心操作    
1. 有价值的数据来源 
> 用户访问行为数据  
> 业务数据  
> 外部第三方数据  
                
2. 数据存储  
    * 存储数据的内容
1. 原始数据、指的是是没有任何操作的数据！-----建议工作中应该备份！

2. 预处理后数据、 编译之后的数据、加密之后的数据

3. 模型结果 一个算法 --通过数据集套用算法得到结果----预测未来！

* 存储数据的设施  
> mysql、Oracle  
> HDFS（分布式）  
> HBase(非关系数据库db)  
> Solr（全文检索）  
> Elasticsearch（基于任务调度）
> Kafka、Redis（缓存数据）---B2C等
> 数据收集 ：Flume & Kafka---

## 数据清洗和转换
实际生产环境中机器学习比较耗时的一部分  
大部分的机器学习模型所处理的都是**特征**，特征通常是**输入变量**所对应的可用于模型的**数值表示**  
大部分情况下 ，收集得到的数据需要经过预处理后才能够为算法所使用  
### 预处理的操作主要包括: 
1. 数据**过滤**  
  + 通过**流的模式**过滤数据  
  + 通过过滤降低数据集与数据集之间的耦合性、兼容性<数据排斥>    
2. 处理数据缺失(**重点**)  
  + 缺失数据是不完整的数据   
3. 处理可能的异常    
  - 先有缺失数据，再有异常数据,在缺失数据里面筛选异常数据~   
  - 有异常的数据是可以使用的,但是必须有条件限制   
  - 单独建立一个异常数据集库，针对于和异常数据有关系的去获取   
  - **异常数据价值非凡**  
4. 错误或者异常值  
5. 合并多个数据源数据    
  常见的数据源有
    + c3po  
    + datasouce(大量使用)   
    + dbcp   
    + jdni  
          
6. 数据汇总  

数据的分类：
> 1. 结构化数据
>     * 关系型数据 mysql oracle--机器可以直接去使用，不需要直接转化,除非特殊情况
> 
> 2. 非结构化数据
>     * mp3
>     * 音频
>     * 视频
>     * 大图片
> 3. 半结构化数据
>     * 日志数据   

### 初步预处理
建立机器学习的向量和矩阵  
方法 

1. 将类别数据编码成为**对应的数值表示**(一般使用1-of-k方法)
2. 从文本数据中提取有用的数据(一般使用**词袋法**或者**TF-IDF**)
3. 处理图像或者音频数据(像素、声波、音频、振幅等<傅里叶变换>)
4. 数值数据转换为类别数据以减少变量的值

## 文本数据抽取

词袋法

* 将文本当作一个**无序的数据集合**，文本特征可以采用文本中的词条T进行体现，那么文本中出现的所有词条及其出现的次数就可以体现文档的特征 ！

TF-IDF

* 词条的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降
* 也就是说词条在文本中出现的次数越多，表示该词条对该文本的重要性越高，词条在所有文本中出现的次数越少，说明这词条对文本的重要性越高
* TF(词频)指某个词条在文本中出现的次数，一般会将其进行归一化处理(该词条数量/该文档中所有词条数量)
* IDF(逆向文件频率)指一个词条重要性的度量，一般计算方式为总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到
* TF-IDF实际上是：TF * IDF                  
            