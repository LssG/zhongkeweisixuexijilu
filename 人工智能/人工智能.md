# 人工智能

## 人工智能的技术体系

思考 ？什么是软件 ？  
  其实软件就是一行**数据**  
  数据如何产生？
   
## 开发一个软件的流程

1. 需求分析

2. 静态html页面原型  
不带数据视图view  
技术体系：html5、css3、JQuery、aJax、js(框架)-Node.js

2. 数据库建模  
简写为：UML 技术    
一般利用第三方工具    
开发中最重要的技术  
数据库是：**关系**型数据库--关系就是：项目的业务逻辑  
常用数据库：Mysql Oracle  

4. 后台编码开发阶段
常用java的框架：ssm、ssh、struts、Hibernate、Spring、Springmvc、Mybatis
基于微服务的框架：Springboot、Springcloud 

5. 测试
包括功能测试 、压力测试、黑盒测试 、白盒测试 .....                        
> 面试题：黑盒就是功能测试，不能测试代码核心
>        白盒测试：测试内核 代码--成本很高！
       
6. 部署到服务器里面  < www >  
3大云服务器：阿里云、腾讯云、七斗云  
云服务器优点：免费维护！


7. 维护 

收集数据--**数据集**--人工智能的核心就是：处理 **数据集**
           
## 机器学习的流程

i  :机器需要思考--算法  
ii :人需要采集数据  
iii:推广数据的采集找出训练数据--产生机器学习的数据集  
## 机器学习的定义


  i：机器学习是一门从**数据**中研究**算法**的**科学**学科。         
  
  ii：人工智能目前面临的最大问题是数据没有办法获取?  
  重点：**数据的来源有哪些？**
>   1. 来源于 项目数据库数据
>   2. 来源于第三方数据--接口数据
>   3. 缓存数据          
  
numpy的核心是多维度空间（矩阵）

## 机器学习理性认识   
输入： x （属性值）  
输出： y  (目标值 )  
f : X -> Y(理想的公式)  
输入数据：历史记录信息  
g : X -> Y(学习得到的最终公式 )   
d={(x1,y1,z1......),(xn,yn,zn......)}--数据集  
 
机器：1+1 = ？  思考 ？机器快还是人快？  
1000\*12121211398394839=？   
1000\*12121211398394839的n次方呢？  
总结：机器学习只能处理业务数据较为繁琐的数据集！不适合处理业务数据简单的数据！  

## 机器学习的概念
对于某给定的*任务T*，在合理的*性能度量方案P*的前提下，某计算机程序可以自主学习任务T的*经验E*；随着提供合适、优质、大量的经验E，该程序对于任务T性  
> 任务T(机器需要做什么？制作山西刀削面)--性能度量方案P(机器如何削面)--经验E(过了一个月，机器总结经验)--下次直接削面（直接执行任务）

算法(T)：根据业务需要和数据特征选择的相关算法,也就是一个数学公式 
> 例如：线性回归、随机森林、决策树、ID3算法、高斯分布....

模型(E)：基于数据和算法构建出来的模型 
> 结合算法 +数据集<给大家提供>   ===构建一个模型

评估/测试(P)：对模型进行评估的策略
>将模型产生的数据集进行 评估 --为什么要评估数据？

选择最优数据，摈弃有噪点的数据！

## 机器学习框架
**重点**  

1. sciket-learn(Python)  
http://scikit-learn.org/stable/
基于Python的人工智能开发--Python属于轻量级--企业大量使用  
> 可以使用java进行人工智能开发，但是java语言属于重量级语言，不适合研发AI产品。
> > 重量级,轻量级--针对于服务器读取数据的并发！(存疑)
  
  
  
2. Mahout(Hadoop生态圈基于MapReduce)  
http://mahout.apache.org/
基于大数据的人工智能
目前有阿里、jd、等大型B2C项目使用
> 小公司不适合利用,因为数据量不是 很大，成本很高！  

 
3. 基于spark的人工智能  
http://spark.apache.org/         
目前技术要求最高  
处理数据速度比前两个快的
             
## 机器学习的应用商业场景 
1. 个性化推荐：  
个性化指的是根据各种因素来改变用户体验和呈现给用户内容这些因素可能包含**用户的行为数据**和**外部因素**；推荐常指系统向用户呈现一个用户可能感兴趣的物品列表。  
> 用户的行为数据  
> 即用户直接操作的数据  
> 行为数据有价值  
> 在人工智能里面检索实现的原理是**分词器**
       
2. 精准营销：
从用户群众中找出特定的要求的营销对象           
指的是通过挖掘客户群体的软点数据或者客户数据的损失数据来自动匹配相关的数据集
3. 客户细分：
试图将用户群体分为**不同的组**，根据给定的**用户特征**进行客户分组。     
**用户特征**   **不同的组**  的核心思想就是 ：oop

4. 预测建模及分析：  
 根据已有的数据进行建模，并使用得到的模型预测未来。
 扩展：模式识别   
 
## 数据分析、数据挖掘、机器学习的区别
**重点**  

1. 数据分析：  
数据分析是指用适当的统计分析方法对收集的**大量数据**进行分析，并提取**有用的信息**，以及**形成结论**，从而对数据进行**详细的研究**和**概括过程**。在实际工作中，数据分析可帮助人们**做出判断**；--减小数据的误差！（？）
切记：数据分析一般而言可以分为统计分析(基于Python的数据分析)、 探索性数据分析和验证性数据分析（验证数据的合法性、数据的安全性等......）三大类。  
    
2. 数据挖掘：  
一般指从大量的数据中通过算法**搜索隐藏**于其中的信息的过程。  
程序员也不可能通过插件开发获取到<只能利用算法实现搜索隐藏的数据>  
通常 通过统计、检索、机器学习、模式匹配等诸多方法来实现这个过程。
> 隐藏数据指的是用户自己/程序员无法手动获取  
 
3. 机器学习就是 整合   i  和 ii

## 机器学习的分类    
**重点**  

1. 有监督学习  
利用已经建立好的模型model来预测未来的样本数据  
有监督学习的核心思想是给数据加标签，即是标签化数据   
2. 无监督学习  
指的是没有标签化的数据操作，直接通过机器处理数据，但是这样的误差会大，但是可以推导出很多数据的内部结构
> 内部结构指的是数据的筛选操作

3. 半监督学习  
指的是利用少量的已结标注的数据和大量未标注的数据进行操作  
主要考虑如何利用少量的标注样本和大量的未标注样本进行训练和分类的问题。  
半监督学习对于减少标注代价，提高学习机器性能具有非常重大的实际意义。  
SSL的成立依赖于模型假设，主要分为三大类：**平滑假设**、**聚类假设**、**流行假设**； 其中流行假设更具有普片性。  
SSL类型的算法主要分为四大类：**半监督分类、半监督回归、半监督聚类、半监督降维**。   
缺点：抗干扰能力弱，仅适合于实验室环境，其现实意义还没有体现出来；未来的发展主要是聚焦于新模型假设的产生。

## 机器学习由开始到结束的过程
第一步：将数据分类  
通过分类模型，将样本数据集中的样本映射到某个给定的类别中

第二步： 聚类  
通过聚类模型，将样本数据集中的样本分为几个类别，属于同一类别的样本相似性比较大  --误差变小

第三步：回归  
反映了样本数据集中样本的属性值的特性，通过函数表达样本映射的关系来发现属性值之间的依赖关系  --数据的密度值--离散点

第四步： 关联规则  
即可以根据一个数据项的出现推导出其他数据项的出  现频率。  P（A）--预测未来

## 机器学习深度算法 
* **C4.5** 分类决策树算法，决策树的核心算法，
* **ID3** 算法的改进算法。 
* **CART** 分类与回归树(Classification and Regression Trees) 
* **kNN** K近邻分类算法；如果一个样本在特征空间中的k个最相似的样本中大多数属于某一个 类别，那么该样本也属于该类别 
* **NaiveBayes** 贝叶斯分类模型；该模型比较适合属性相关性比较小的时候，如果属性相关性比较大的时候，决策树模型比贝叶斯分类模型效果好(原因：贝叶斯模型假设属性之间是互不影响的) 
* **SVM** 支持向量机，一种有监督学习的统计学习方法，广泛应用于统计分类和回归分析中。 
* **EM** 最大期望算法，常用于机器学习和计算机视觉中的数据集聚领域 
* **Apriori** 关联规则挖掘算法 
* **K-Means** 聚类算法，功能是将n个对象根据属性特征分为k个分割(k < n); 属于无监督学习 
* **PageRank** Google搜索重要算法之一 
* **AdaBoost** 迭代算法；利用多个分类器进行数据分类

## 人工智能和机器学习的区别

机器学习比较简答,是基于算法的实现  
先学习机器学习/大数据之后学习深度学习很简单  
深度学习是核心，难度较大  

深度学习是机器学习的子类；
> 深度学习是基于**传统的神经网络算法**发展到**多隐层的一种算法体现**。

## 机器学习开发流程
**重点内容**

1. 数据收集/数据采集   

2. 数据**预**处理  


3. **特征**提取  
特征的分类 
> 1. 基于行为特征  
> 2. 数据本身的特征 
              
4. 模型构建 
就是通过很多算法构建一个模型,用于存放数据集得到的结果
 
5. 模型测试评估

6. 投入使用(模型部署与整合) 
模型的部署需要硬件的支持

 
7. **迭代**优化  
需要借助于工具   
优化性能问题
      
## 数据的核心操作    
1. 有价值的数据来源 
> 用户访问行为数据  
> 业务数据  
> 外部第三方数据  
                
2. 数据存储  
    * 存储数据的内容
1. 原始数据、指的是是没有任何操作的数据！--建议工作中应该备份！

2. 预处理后数据、 编译之后的数据、加密之后的数据

3. 模型结果 一个算法 --通过数据集套用算法得到结果--预测未来！

* 存储数据的设施  
> mysql、Oracle  
> HDFS（分布式）  
> HBase(非关系数据库db)  
> Solr（全文检索）  
> Elasticsearch（基于任务调度）
> Kafka、Redis（缓存数据）--B2C等
> 数据收集 ：Flume & Kafka--

## 数据清洗和转换
实际生产环境中机器学习比较耗时的一部分  
大部分的机器学习模型所处理的都是**特征**，特征通常是**输入变量**所对应的可用于模型的**数值表示**  
大部分情况下 ，收集得到的数据需要经过预处理后才能够为算法所使用  
### 预处理的操作主要包括: 
1. 数据**过滤**  
  + 通过**流的模式**过滤数据  
  + 通过过滤降低数据集与数据集之间的耦合性、兼容性<数据排斥>    
2. 处理数据缺失(**重点**)  
  + 缺失数据是不完整的数据   
3. 处理可能的异常    
  - 先有缺失数据，再有异常数据,在缺失数据里面筛选异常数据~   
  - 有异常的数据是可以使用的,但是必须有条件限制   
  - 单独建立一个异常数据集库，针对于和异常数据有关系的去获取   
  - **异常数据价值非凡**  
4. 错误或者异常值  
5. 合并多个数据源数据    
  常见的数据源有
    + c3po  
    + datasouce(大量使用)   
    + dbcp   
    + jdni  
          
6. 数据汇总  

数据的分类：
> 1. 结构化数据
>     * 关系型数据 mysql oracle--机器可以直接去使用，不需要直接转化,除非特殊情况
> 
> 2. 非结构化数据
>     * mp3
>     * 音频
>     * 视频
>     * 大图片
> 3. 半结构化数据
>     * 日志数据   

### 初步预处理
建立机器学习的向量和矩阵  
方法 

1. 将类别数据编码成为**对应的数值表示**(一般使用1-of-k方法)
2. 从文本数据中提取有用的数据(一般使用**词袋法**或者**TF-IDF**)
3. 处理图像或者音频数据(像素、声波、音频、振幅等<傅里叶变换>)
4. 数值数据转换为类别数据以减少变量的值

## 文本数据抽取

词袋法

* 将文本当作一个**无序的数据集合**，文本特征可以采用文本中的词条T进行体现，那么文本中出现的所有词条及其出现的次数就可以体现文档的特征 ！

TF-IDF

* 词条的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降
* 也就是说词条在文本中出现的次数越多，表示该词条对该文本的重要性越高，词条在所有文本中出现的次数越少，说明这词条对文本的重要性越高
* TF(词频)指某个词条在文本中出现的次数，一般会将其进行归一化处理(该词条数量/该文档中所有词条数量)
* IDF(逆向文件频率)指一个词条重要性的度量，一般计算方式为总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到
* TF-IDF实际上是：TF * IDF                  
            
## 模型训练及测试

1. 模型选择
    + 最优建模方法:指的是给机器找一个适合数据集的算法公式
    + 最佳参数:指的是是线性回归的参数

2. 在训练数据集上运行模型(算法)并在测试数据集中集中测试效果，迭代进行数据模型的修改，这种方式被称为交叉验证(将数据分为训练集和测试集，使用训练集构建模型，并使用测试集评估模型提供修改建议)    
> 交叉验证是目前训练数据、测试数据、模型的修改和建立中大量使用的方法
3. 模型的选择会尽可能多的选择算法进行执行，并比较执行结果

4. 模型的测试  
常用指标：准确率/召回率/精准率/F值   
* 准确**率**(Accuracy)=提取出的正确样本数/总样本数   
* 召回率(Recall)=正确的正例样本数/样本中的正例样本数——覆盖率
* 精准率(Precision)=正确的正例样本数/预测为正例的样本数
* F值=Precision*Recall*2 / (Precision+Recall) (即F值为正确率和召回率的调和平均值) 

*人工智能的核心：将数学的思想通过算法实现*


5. ROC  
ROC（Receiver Operating Characteristic）最初源于20世纪70年代的信号检测理论，描述的是分类混淆矩阵中FPR-TPR两个量之间的相对变化情况，ROC曲线的纵轴是**真正例率**（True Positive Rate 简称TPR），横轴是**假正例率** （False Positive Rate 简称FPR）。 如果二元分类器输出的是对正样本的一个分类概率值，当取不同阈值时会 得到不同的混淆矩阵，对应于ROC曲线上的一个点。那么ROC曲线就反映了 FPR与TPR之间权衡的情况，通俗地来说，即在TPR随着FPR递增的情况下，谁 增长得更快，快多少的问题。TPR增长得越快，曲线越往上屈，AUC就越大， 反映了模型的分类性能就越好。当正负样本不平衡时，这种模型评价方式比起一般的精确度评价方式的好处尤其显著。
         

6. AUC  
AUC AUC的值越大表达模型越好 AUC（Area Under Curve）被定义为ROC曲线下的面积，显然这个面积的数值不会大于1。又由于ROC曲线一般都处于y=x这条直线的上方，所以AUC的取值范围在0.5和1之间。使用AUC值作为评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好， 而AUC作为**数值**可以直观的评价**分类器**的好坏，值越大越好。--对于数据的分析而言就比较理想的！
* AUC = 1，是完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。
* 0.5 < AUC < 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。 
* AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。 
* AUC < 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测
 
 

## 模型评估 
1. **回归**结果 **度量** explained_varicance_score
2. 可解释方差的回归评分函数   mean_absolute_error
3. 平均绝对**误差** mean_squared_error：平均平方误差  --        
    
## 数据集的存放
* 关系型数据库(mysql、Oracle、db2、sqllite、）和非关系型数据库(Hbase、radis .....)
  -  mysql 
>   适合于研发桌面集应用程序开发，例如：教务管理系统  、图书管理系统、[POST收银] 、app数据、网站的数据存放  
>   mysql安全系数较高，内存空间较大，数据的读写分离速度较快
  - Oracle  
>   Oracle基于海量数据的处理比较多。适合于大型企业级项目研发 ：金融<银行、保险、医疗 、交通>  
>   Oracle成本太高，  
>   存放海量数据是很安全的，数据容易维护，Oracle底层是一种plsql语言组成，内存空间大，数据的读写速度较快  
>   Oracle产生的数据集是加密的
             
  - DB2
>   IBM公司
>   存放数据量最大、但是收费极高,数据非常安全
        
  - sqllite 
>   默认内存为4M  
                     
## Oracle数据集的操作机制

安装Oracle 
**难点**,**重点**            


## 线性回归的模型建立步骤
 
1. 线性回归<非线性回归>需要导入的库   
``` 
from sklearn.model_selection import train_test_split  
from sklearn.linear_model import LinearRegression  
from sklearn.linear_model import Lasso  
from sklearn.linear_model import Ridge  
from sklearn.linear_model import ElasticNet  
from sklearn.preprocessing import PolynomialFeatures, StandardScaler  
from sklearn.pipeline import Pipeline  
```
> 目前机器学习有以上7个包足够--可以开发：聊天机器人、家教机、 肢体动作语言的解析


2. 定义函数，将数据集存放到函数里面--去建立模型**建立矩阵**

3. 定义read方法/函数  
机器学习的数据集要求尽量选择csv    
> csv文件处理数据的性能好。数据好维护！  
> 2：csv得到的测试数据集可以减少数据集的异常 、错误值 、缺省值！ 

4. x,y的变化--得到一个模型/算法
 
5. 绘制数据模型图 
> 绘制前调整数据
    
6. 将数据集进行拟合曲线的比较  
减小数据的误差   
去除数据的噪点  
    
## 线性回归
有LASSO回归、Ridge回归  
LASSO具有稀疏作用，Ridge收敛更快   

我们说，目标函数仍然是不带正则化的原函数，经过改造的上式称为损失函数  

Ridge回归  
备注：损失函数利用比较广泛  
    
## logistic回归  
**重点内容**

logistic回归是最基础的**分类算法**  
备注：如果数据集要求利用线性回归去操作--尽量使用分类算法(logistic回归)  
    回顾伯努利分布，一次实验的结果只有0、1两种选择  
    根据贝叶斯公式，如果只考虑P(A|B)，则称为极大似然估计  
   
logistic回归是一种分类算法  
特性：  
+ 模型简单、计算量较小--指的是适合于大部分数据操作，目前企业大量使用  
+ 对异常数据点并不敏感--对数据集的节点不是很敏感--但是目前不影响机器学习  
+ 对数据预处理要求较高--预编译数据越高，数据集越安全--目前企业大量使用  

总结：logistic回归是算法利大于弊 

## 决策树
**重点内容**
决策树能用来做回归，也可以用来做分类  
是一类算法的总称  
 
决策树是描述对数据进行分类的树形模型，可以是二叉树或非二叉树，内部节点(绿色)表示一个特征或属性，叶子节点(橘色)表示一个结果类。  

在做回归任务时， **以叶子节点（子节点）的值**指代输出值。  
和一个封装好的 Dtree--父节点<根节点>--子节点<采集的数据集>  

## 朴素贝叶斯和随机森林马尔科夫模型
朴素贝叶斯和随机森林马尔科夫模型必须的包：
```
from sklearn.naive_bayes import MultinomialNB, BernoulliNB   #封装了机器码
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer#？自然语言的处理包
from sklearn.model_selection import GridSearchCV
from sklearn import metrics
```

## 人工智能深度学习、深度学习发展

+ 感知机  
  对生物神经细胞的简单数学模拟，是最简单的人工神经网络，**只有一个神经元**。   
  感知机也可以看做是“线性分类器的一个经典学习算法”。  
  备注：感知机的缺陷：破坏神经元数据集的结构  
  
+ 深度学习发展
         前馈神经网络 --基于神经元
         < input \>通过前馈神经网络--将数据集信息传递给神经元组织--作出反映< output \>  
         操作前馈神经网络注意事项
          1：增加input尺寸--指的是将所有的前馈神经网络模型用去刺激神经元--数据集很乱  
          2：增加隐层数目--暴露出隐藏在内部的神经元--用于计算数据集--不建议--激活神经元  
          3：全连接网络--可以完成人不能完成的高难度的工作。  
      
+ 深度学习框架  
TensorFlow是谷歌基于DistBelief进行研发的第二代人工智能学习系统  
其命名来源于本身的运行原理。**Tensor（张量）意味着N维数组**，**Flow(流)**意味着基于**数据流图**的计算  
TensorFlow为张量从流图的一端流动到另一端计算过程  
TensorFlow是将**复杂的数据结构传输至人工智能神经网** 中进行分析和处理过程的系统  
TensorFlow的3大特性:
> i :高度的灵活性--高度 ：指的是可以解析各种多样的数据流图  
> ii真正的可移植性(server/mobile)  
> iii多语言支持(python/c++/Go/java/....)--跨平台的特性  


## 卷积神经网络

